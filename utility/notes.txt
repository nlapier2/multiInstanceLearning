"CAMIL": Clustering and Assembly with Multiple Instance Learning
CAMIL H-BoW and CAMIL D-BoW
#$ -q all-LoPri.q

Original 60: SRR413*, where * is 556-557, 646-664, 666-682, 684-702, 757-759

Size of all 367 patient files in FASTQ format: 4.9TB = (4900 - (690+42+626+250)) GB = 3292 GB = 3.29 TB
Gotten with du -sh. Average patient file size: (3292 / 367) GB = 8.97 GB
After assembly: 25.18 GB total size

367 patients: 182 diabetic, 185 healthy
344 patients: 170 diabetic, 174 healthy
23 patients: 12 diabetic, 11 healthy

Cluster run time: 14 hours, 7 minutes, 44 seconds
SVM-light run time: 9 minutes, 13 seconds for D-BoW; 8 minutes, 49 seconds for H-BoW
Write vectors run time: 24 hours, 8 minutes, 49 seconds

GICF ideal parameters: 5 iterations, 0.0005 learning rate, 0.01 mini batch size (top k = 0.9)

--------------------------------

python pipelineT2D.py -v --dir /home/nathan/school/research/multiInstanceLearning/cirrhosisTests --input cirrhosis.fasta --map cirrhosis.map --positive Encephalopathy --negative 'No Encephalopathy' --cluster uclust --svm misvm --result_dir results/option4-misvm-encVSno/ --output resultFile --split -1 --kmer 4 --write small_vectors

python gicf.py -v -i /home/nathan/school/research/multiInstanceLearning/cirrhosisTests/results/option4-misvm-encVSno/vectors

--------------------------------

K = 41 or K = 51 with read size limited to 100 seemed to work best. I chose K = 51.

--------------------------------

GeneMark: done, not sure how to use yet

Explicit association of clusters to labels with SVM-light
- take the prediction file and augment with explicit association with cluster labels and an example
- maybe augment with genemark results to predict genes that are most indicative of having or not having disease

Start writing my paper
- clarify direction of paper
- start with parts of introduction and related work sections from class paper, maybe some of methods (UCLUST, SVM-light, etc)
- remember matplotlib for charts: http://matplotlib.org/

Same data as MGWAS paper?
- can start downloading other patients with bulk downloader, but still not all data is available
- may have to ask paper authors where to find all data

--------------------------------

EXPERIMENTS for GICF (accuracy and F1 [maybe also runtime])

Number of iterations: 1, 3, 5, 10, 20
Mini-batch size: 0.01, 0.05, 0.1, 0.25, 1
(Not done) Selected from: Top 2%, Top 10%, Top 20%, Top 50%, All
Learning Rate: 0.00001, 0.00005, 0.0001, 0.0005, 0.001
Top-K: Yes or No
Kernel (Similarity): Yes or No
Part of Dataset: 1% or All

Since mini batches are randomized (as are weights), run each experiment 3-5 times to get range of results
Do training set as test set to see accuracy of that
Do both Cirrhosis and T2D datasets (must first assemble Cirrhosis data)

--------------------------------

  /* added code */
  int iter;
  double *wtemp = model->lin_weights;
  fprintf(predfl, "[");
  for(iter=0; iter < len; iter++){
    fprintf(predfl,"%lf", *wtemp);
    wtemp++;
    if(iter+1 < len)  fprintf(predfl,", ");
  }
  fprintf(predfl, "]\n");
  /* end added code */

1317: number of clusters from assembled fasta file

--------------------------------

k=4, out1
Best parameter settings [learn rate, lambda, mini batch size, descent iterations]: [0.0001, 60.0, 0.9, 10]
Average prediction: 0.762984045957
Best results: 
Accuracy: 0.633333333333
Precision and recall: 0.590909090909 and 0.866666666667
F1 Score: 0.702702702703
True Positive Rate and False Positive Rate: 0.866666666667 and 0.6


k=3, out1
Best parameter settings [rate, lambda, top_k, mini batch size, iterations]:
[0.0001, 60.0, 0.9, 0.01, 30]
Best results: 
Accuracy: 0.6
Precision and recall: 0.555555555556 and 1.0
F1 Score: 0.714285714286
True Positive Rate and False Positive Rate: 1.0 and 0.8


k=3, out1
Best parameter settings [rate, lambda, top_k, mini batch size, iterations]:
[5e-05, 60.0, 0.9, 0.01, 30]
Best results: 
Accuracy: 0.633333333333
Precision and recall: 0.583333333333 and 0.933333333333
F1 Score: 0.717948717949
True Positive Rate and False Positive Rate: 0.933333333333 and 0.666666666667


k=3, out1
Best parameter settings [rate, lambda, top_k, mini batch size, iterations]:
[0.0001, 60.0, 0.9, 0.01, 3]
Best results: 
Accuracy: 0.7
Precision and recall: 0.6875 and 0.733333333333
F1 Score: 0.709677419355
True Positive Rate and False Positive Rate: 0.733333333333 and 0.333333333333

k=3, out1
Best parameter settings [rate, lambda, top_k, mini batch size, iterations]:
[0.0001, 60.0, 0.9, 0.01, 30]
Best results: 
Accuracy: 0.566666666667
Precision and recall: 0.535714285714 and 1.0
F1 Score: 0.697674418605
True Positive Rate and False Positive Rate: 1.0 and 0.866666666667


--------------------------------


Pipeline (CAMIL) - Both H-BoW and D-BoW versions
MI-SVM: shows why Instance Space (IS) and Standard MIL assumption are bad (also do sbMIL?)
GICF: avoids standard MIL assumption and learns instance labels, but still IS
EMD+SVM?: Bag Space (BS) example, show why vocabulary needed in bioinformatics?
R packages from mgwas paper?: our way of comparing to original paper

Then: compare CAMIL to original paper results and explain why better (de novo, less training data)

Then: show CAMIL instance "labels" better than GICF (CAMIL should have some clusters labeled as clearly bad or good, whereas GICF labels not very differentiated)
Voting Framework (VF) from key instance detection paper?: like GICF but standard MI assumption




--------------------------------


TP:8
FP:0
TN:11
FN:4

Precision = TP / (TP + FP) = 8 / 8 = 1.0
Recall = TP / (TP + FN) = 8 / 12 = 0.667
F1 Score = 2 * 1 * 0.667 / (1 + 0.667) = 0.8002



--------------------------------


>>> a[-25:]
[0.009657, 0.009664, 0.009795, 0.009816, 0.009827, 0.009866, 0.009888, 0.00997, 0.009981, 0.009984, 0.010299, 0.010428, 0.010497, 0.010527, 0.010528, 0.010534, 0.010544, 0.010639, 0.010648, 0.010692, 0.010714, 0.010891, 0.011196, 0.011356, 0.012109]
>>> a[:25]
[-0.003668, -0.003551, -0.003546, -0.0031, -0.003083, -0.002915, -0.0029, -0.002899, -0.002852, -0.002782, -0.002708, -0.002693, -0.002603, -0.002581, -0.002536, -0.002519, -0.002502, -0.002492, -0.002476, -0.002439, -0.002431, -0.00241, -0.002391, -0.002371, -0.002366]



--------------------------------


#$ -q all-LoPri.q

D-BoW final performance (even split):
Accuracy on test set: 87.43% (160 correct, 23 incorrect, 183 total)
Precision/recall on test set: 83.84%/92.22%
F1 Score = 2*.8384*.9222 / (.8384+.9222) = 87.83%
Leave-one-out estimate of the error: error=16.30%
Leave-one-out estimate of the recall: recall=90.22%
Leave-one-out estimate of the precision: precision=79.81%
7 minutes and 59 seconds

H-BoW final performance (even split):
Accuracy on test set: 93.99% (172 correct, 11 incorrect, 183 total)
Precision/recall on test set: 98.77%/88.89%
F1 Score = 2*.9877*.8889 / (.9877+.8889) = 93.57%
Leave-one-out estimate of the error: error=11.96%
Leave-one-out estimate of the recall: recall=80.43%
Leave-one-out estimate of the precision: precision=94.87%
7 minutes and 21 seconds


D-BoW no feature engineering:
Accuracy on test set: 86.34% (158 correct, 25 incorrect, 183 total)
Precision/recall on test set: 80.95%/94.44%
F1 Score = 2*.8095*.9444 / (.8095+.9444) = 87.18%
Leave-one-out estimate of the error: error=17.93%
Leave-one-out estimate of the recall: recall=88.04%
Leave-one-out estimate of the precision: precision=78.64%
9 minutes, 13 seconds

H-BoW no feature engineering:
Accuracy on test set: 90.71% (166 correct, 17 incorrect, 183 total)
Precision/recall on test set: 98.67%/82.22%
F1 Score = 2*.9867*.8222 / (.9867+.8222) = 89.70%
Leave-one-out estimate of the error: error=10.87%
Leave-one-out estimate of the recall: recall=80.43%
Leave-one-out estimate of the precision: precision=97.37%
8 minutes, 49 seconds


--------------------------------

>>> plt.subplot(111)
<matplotlib.axes._subplots.AxesSubplot object at 0x7fa53abb4710>
>>> plt.ylabel("Cluster weight")
<matplotlib.text.Text object at 0x7fa51c450d90>
>>> a[:10]
[-0.000333, -0.000291, -0.000239, -0.000215, -0.00019, -0.000189, -0.000188, -0.000182, -0.000164, -0.000158]
>>> a[-10:]
[0.000147, 0.000153, 0.000166, 0.00017, 0.000174, 0.000191, 0.000242, 0.000248, 0.000341, 0.000374]
>>> plt.xlim(0,4000)
(0, 4000)
>>> plt.ylim(-0.0004, 0.0004)
(-0.0004, 0.0004)
>>> for i in range(len(a)):
...     plt.scatter(i, a[i])
>>> plt.savefig('scatter.png')



>>> c = []
>>> for i in range(len(a)):
...   c.append(i)
... 
>>> d = []
>>> for i in range(len(a)):
...   d.append([c[i], a[i]])
... 
>>> d.sort(key=lambda x: x[1])
>>> print d[0]
[561, -0.000333]
>>> print d[3917]
[749, 0.000374]


--------------------------------


- for table on 23 test patients, randomly select 5 or 10 different subsets of 23 and average the results
- add table on subset of instances, so we can compare MISVM and sbMIL, along with explanation


--------------------------------

Results for random 23 test sets:

--- Original ---
HT14A
ED11A
HT25A
ED10A
HT8A
T2D 189A
T2D 198A
T2D 206A
N075A
SZEY 101A
T2D 207A
SZEY 103A
SZEY 93A
SZEY 106A
ED16A
T2D 192A
T2D 195A
N104A
SZEY 95A
SZEY 90A
SZEY 97A
SZEY 99A
SZEY 104A
mRMR + SVM: 82.61 Acc, 82.76 F1, 87.12 AUC
CAMIL D-BoW: 91.30 Acc, 92.31 F1, 100.00 AUC
CAMIL H-BoW: 100.00 Acc, 100.00 F1, 100.00 AUC


--- Random 1 ---
SRR341672
SRR341716
SRR413593
SRR341598
SRR341587
SRR413649
SRR413744
SRR413637
SRR413728
SRR413769
SRR413659
SRR413579
SRR341606
SRR341651
SRR413621
SRR413647
SRR413611
SRR413708
SRR413585
SRR413602
SRR413746
SRR341607
SRR341720
mRMR + SVM: 86.95 Acc, 82.57 F1, 86.92 AUC
CAMIL D-BoW: 95.65 Acc, 95.24 F1, 100.00 AUC
CAMIL H-BoW: 95.65 Acc, 94.74 F1, 100.00 AUC


--- Random 2 ---
SRR413656
SRR413590
SRR413677
SRR341595
SRR341695
SRR413572
SRR341623
SRR341706
SRR413613
SRR413654
SRR341658
SRR341588
SRR413568
SRR341603
SRR341615
SRR413624
SRR413759
SRR413619
SRR341626
SRR413609
SRR413736
SRR341630
SRR413723
mRMR + SVM: 82.61 Acc, 83.33 F1, 84.84 AUC
CAMIL D-BoW: 82.61 Acc, 81.82 F1, 90.91 AUC
CAMIL H-BoW: 86.96 Acc, 84.31 F1, 90.15 AUC


--- Random 3 ---
SRR413566
SRR413651
SRR413734
SRR413661
SRR413731
SRR341599
SRR413729
SRR341602
SRR413680
SRR413632
SRR341618
SRR341591
SRR341591
SRR413600
SRR413672
SRR413725
SRR341725
SRR341666
SRR413571
SRR413597
SRR413740
SRR413701
SRR413600
mRMR + SVM: 73.91 Acc, 78.79 F1, 78.46 AUC
CAMIL D-BoW: 90.48 Acc, 91.67 F1, 98.15 AUC
CAMIL H-BoW: 95.24 Acc, 95.65 F1, 97.22 AUC


--- Random 4 ---
SRR341656
SRR341667
SRR413747
SRR341584
SRR341679
SRR413642
SRR341639
SRR341702
SRR341706
SRR413578
SRR413768
SRR341658
SRR413609
SRR341608
SRR413645
SRR341692
SRR341635
SRR341712
SRR413764
SRR341693
SRR413721
SRR413689
SRR341719
mRMR + SVM: 82.61 Acc, 78.26 F1, 85.38 AUC
CAMIL D-BoW: 91.30 Acc, 90.91 F1, 95.38 AUC
CAMIL H-BoW: 95.65 Acc, 95.24 F1, 97.69 AUC


--- Random 5 ---
SRR413740
SRR413643
SRR341633
SRR341627
SRR413576
SRR413678
SRR413733
SRR413717
SRR413678
SRR413580
SRR413751
SRR413614
SRR413621
SRR413666
SRR1778454
SRR413748
SRR1778450
SRR413735
SRR413727
SRR341605
SRR413644
SRR413641
SRR341675
mRMR + SVM: 73.91 Acc, 78.79 F1, 76.92 AUC
CAMIL D-BoW: 95.45 Acc, 96.00 F1, 100.00 AUC
CAMIL H-BoW: 100.00 Acc, 100.00 F1, 100.00 AUC


--- Random 6 ---
SRR341656
SRR413594
SRR341663
SRR341718
SRR341688
SRR413557
SRR1778453
SRR341640
SRR413613
SRR413728
SRR341615
SRR413754
SRR413616
SRR341696
SRR413684
SRR413616
SRR413574
SRR341701
SRR413565
SRR341643
SRR341595
SRR413724
SRR413751
mRMR + SVM: 73.91 Acc, 75.86 F1, 77.27 AUC
CAMIL D-BoW: 95.45 Acc, 95.65 F1, 95.83 AUC
CAMIL H-BoW: 95.45 Acc, 95.65 F1, 95.00 AUC


--- Random 7 ---
SRR341683
SRR413749
SRR413629
SRR341669
SRR341640
SRR413645
SRR341627
SRR413625
SRR341664
SRR413635
SRR413701
SRR413696
SRR413705
SRR341599
SRR341680
SRR413756
SRR413559
SRR341625
SRR413672
SRR341611
SRR413773
SRR413719
SRR341591
mRMR + SVM: 91.30 Acc, 93.33 F1, 92.86 AUC
CAMIL D-BoW: 91.30 Acc, 93.33 F1, 100.00 AUC
CAMIL H-BoW: 91.30 Acc, 92.31 F1, 99.21 AUC


--- Random 8 ---
SRR341649
SRR341586
SRR413613
SRR413643
SRR413690
SRR341640
SRR413709
SRR413565
SRR413583
SRR413601
SRR341632
SRR341582
SRR341612
SRR413730
SRR341651
SRR413585
SRR413640
SRR413724
SRR413608
SRR413627
SRR341602
SRR413763
SRR413649
mRMR + SVM: 78.26 Acc, 78.26 F1, 80.95 AUC
CAMIL D-BoW: 86.96 Acc, 85.71 F1, 100.00 AUC
CAMIL H-BoW: 95.65 Acc, 94.12 F1, 100.00 AUC


--- Random 9 ---
SRR341700
SRR413723
SRR413702
SRR413668
SRR1778452
SRR413743
SRR341678
SRR413740
SRR341670
SRR341620
SRR413759
SRR341599
SRR413683
SRR413650
SRR413707
SRR413627
SRR413564
SRR413611
SRR341593
SRR413608
SRR413609
SRR413575
SRR413724
mRMR + SVM: 73.91 Acc, 80.00 F1, 72.31 AUC
CAMIL D-BoW: 95.65 Acc, 96.30 F1, 100.00 AUC
CAMIL H-BoW: 100.00 Acc, 100.00 F1, 100.00 AUC


--- Random 10 --- [NOT INCLUDED IN AVERAGES]
SRR413660
SRR413631
SRR341692
SRR1778456
SRR341703
SRR413621
SRR341609
SRR413706
SRR413724
SRR341594
SRR413591
SRR341723
SRR413659
SRR341657
SRR341589
SRR413773
SRR413679
SRR341667
SRR341598
SRR341707
SRR413666
SRR413661
SRR341648
mRMR + SVM: 86.96 Acc, 88.00 F1, 94.70 AUC
CAMIL D-BoW: 95.65 Acc, 95.65 F1, 92.42 AUC
CAMIL H-BoW: 91.30 Acc, 90.91 F1, 89.39 AUC



--- Averages ---

mRMR + SVM
Accuracy:
F1 Score:
AUC-ROC :

CAMIL D-BoW
Accuracy:
F1 Score:
AUC-ROC :

CAMIL H-BoW
Accuracy:
F1 Score:
AUC-ROC :

