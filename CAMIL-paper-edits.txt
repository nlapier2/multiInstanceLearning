CAMIL Paper Edits Based on Reviews (minor edits not included)


-----------------------------


Original discussion of feature extraction:
We test both the Distance-based and Histogram-based feature extraction methods within the CAMIL pipeline. For each patient, we initialized a vector with length equal to the number of clusters. The value for each scalar in that patient's feature vector would then represent either the read that matched most closely to that cluster centroid (Distance-based), or the number of reads from that cluster that were present (Histogram-based). An example of this is illustrated as a part of Figure \ref{pipeline}. We implemented this feature extraction method in Python. We performed classification with a standard SVM classifier using the generated feature vectors; in this case, we used svm-light \cite{joachims08}. The choice of classifier is not very important for BoW methods \cite{amores13}.


Edited version:
We used a "vocabulary-based" feature extraction method. An example of Vocabulary-based methods are Bag of Words (BoW) methods, which involve the following three-step process: (i) Cluster the instances to create classes of instances; (ii) for each bag, map the clusters of instances in that bag to a feature vector; and (iii) use a standard classifier that uses the feature vectors to predict group labels \cite{amores13}. Step (i) is covered by our assembly and clustering process, while step (iii) is covered by performing classification with a standard SVM classifier based on the extracted feature vectors. In this case, we used svm-light \cite{joachims08}. Below, we describe our feature selection methods for step (ii), which we implemented in Python, as well as the rationale for using these methods. 
Amores found the Distance-based Bag of Words (D-BoW) method to be the second most effective of all tested methods, and the most effective one that was also time-efficient (linear, rather than quadratic, in the number of bags and number of instances per bag) \cite{amores13}. H-BoW methods were found by Amores to be somewhat less effective than D-BoW methods on average, but performed the best out of all algorithms on several datasets, indicating that this method performs very well on some real world problems \cite{amores13}. Thus, we tested our pipeline using both of these feature extraction methods. 
Either way, the input is a set of clusters for each patient. The D-BoW method creates a feature vector based on the contig for each cluster that was the closest match to the cluster seed. For instance, say Patient A's reads include the centroid of cluster 1, another contig that has a 45\% match to the centroid of cluster 1, no contigs from cluster 2, and two contigs that match to the centroid of cluster 3, one with a 57\% match and one with an 82\% match. The string match percentage is determined by UCLUST, as described in the previous subsection. Then, D-BoW would extract the feature vector [1, 0, 0.82], indicating the contigs for Patient A that match most closely to the cluster centroid for each cluster. The H-BoW method, instead of using the closest match to each cluster, counts the number of contigs for a patient that belong to each cluster. For the above example, the H-BoW method would extract the feature vector [2, 0, 2], since Patient A has 2 representatives from clusters 1 and 3, but no representatives from cluster 2. This example is illustrated in part of Figure \ref{pipeline}.


Original "Overview" subsection within Methods:
Our proposed pipeline involves a number of steps, which serve a variety of purposes. For each patient file, we assembled the sequence reads, which served the dual purpose of generating larger contigs that contain more functional biological information and reducing the dataset size by discarding reads that could not be assembled. The clustering step assigns the contigs to certain clusters, which represent functionally similar microbes, and thus establish classes of instances that can be used as features for the classifier. We then developed a vocabulary-based feature extraction method.
An example of Vocabulary-based methods are Bag of Words (BoW) methods, which involve the following three-step process: (i) Cluster the instances to create classes of instances; (ii) for each bag, map the clusters of instances in that bag to a feature vector; and (iii) use a standard classifier that uses the feature vectors to predict group labels \cite{amores13}. Amores found the Distance-based Bag of Words (D-BoW) method to be the second most effective of all tested methods, and the most effective one that was also time-efficient (linear, rather than quadratic, in the number of bags and number of instances per bag) \cite{amores13}. The distinguishing feature in D-BoW methods is that the values for the feature vector represent the instance that has the smallest distance to the cluster center. Histogram-based Bag of Words (H-BoW) methods count the number of instances from each cluster there are in each bag, instead of keeping track of the closest-matching instance to that cluster. This intuitively has appeal in the domain of microbiome analysis, as the relative quantities of different species of bacteria is important. H-BoW methods were found by Amores to be somewhat less effective than D-BoW methods on average, but performed the best out of all algorithms on several datasets, indicating that this method performs very well on some real world problems \cite{amores13}.
Our feature extraction method uses either the Histogram-based or Distance-based BoW method. We used an SVM-based classifier to predict patient phenotype, and used several metrics to assess its accuracy. We used the SVM's decision boundary to infer information about which clusters of instances were most or least indicative of the phenotype, discussed further in subsection \ref{deriving-instances}. Aside from the patient labels, this process is entirely de novo, and does not consult any external databases. An illustration of the pipeline is shown in Figure \ref{pipeline} on page \pageref{pipeline}.


Edited version:
Our proposed pipeline involves a number of steps, which serve a variety of purposes. For each patient file, we assembled the sequence reads, which served the dual purpose of generating larger contigs that contain more functional biological information and reducing the dataset size by discarding reads that could not be assembled. The clustering step assigns the contigs to certain clusters, which represent functionally similar microbes, and thus establish classes of instances that can be used as features for the classifier. We then developed a vocabulary-based feature extraction method, discussed further in subsection \ref{feature-extraction}. Using the extracted feature vectors, we trained an SVM-based classifier to predict patient phenotype, and used several metrics to assess its accuracy. We used the SVM's decision boundary to infer information about which clusters of instances were most or least indicative of the phenotype, discussed further in subsection \ref{deriving-instances}. Aside from the patient labels, this process is entirely de novo, and does not consult any external databases. An illustration of the pipeline is shown in Figure \ref{pipeline} on page \pageref{pipeline}.


-----------------------------


Removed from Methods->Assembly section:
SOAPdenovo2 first constructs a type of directed graph called a \emph{de Bruijn} graph that represents the overlaps between different sequences \cite{li10}. Reads are divided into strings of length K called \emph{k-mers}; these k-mers are the nodes of the graph \cite{zerbino08}. The choice of K is up to the user, and is important for having good assembly results. The k-mer nodes in the graph have an edge between them if a read contains those k-mers in order with an overlap of K-1 nucleotides, and the direction of the edge indicates in which order the k-mers appear \cite{zerbino08}. SOAPdenovo2 then cleans this graph by removing nodes/sequences with few or no connections with other sequences, eliminating ``tips" that represent likely sequencing machine errors, and removing redundant edges \cite{li10}. This step helps to reduce the overall size of the data by eliminating some reads that would not have been useful anyway. The contigs are then formed by combining reads according to the de Bruijn graph: each contig represents a directed path in the graph \cite{zerbino08}. 


-----------------------------


Original results comparison between CAMIL and mRMR+SVM:
Finally, Table \ref{tab:test-comp} compares CAMIL to the method used by Qin et al., mRMR + SVM. We use 23 patients in the test set here because that is what Qin et al. did \cite{qin041012}. We averaged the results of 10 trials in which we selected 23 patients randomly out of the 367 to serve as the test set, with the other 344 of the training set. Table \ref{tab:test-comp} shows that CAMIL significantly outperformed mRMR + SVM, with the H-BoW variant of CAMIL slightly outperforming the D-BoW variant. The results held for the original 23 patient test set used by Qin et al., where CAMIL H-BoW actually achieved 100\% accuracy.
The method used in the original paper is the only one tested here that is not an MIL method, and the only one that is not entirely unsupervised apart from the patient labels. Given that the methods from this paper were not de novo, it makes sense that they would outperform many of the unsupervised MIL methods. However, CAMIL still significantly outperformed the results reported in the original paper. We believe that this is due to the following reasons: (i) the clustering process puts similar contigs into groups that are useful features for the classifier, and (ii) instead of attempting to select the most significant features before performing classification, we allow the classifier itself to determine the most significant features from the entire pool of features.


Edited version:
Finally, Table \ref{tab:test-comp} compares CAMIL to the method used by Qin et al., mRMR + SVM. We initially tested CAMIL on the same 23 patient test set that Qin et al. used, for which CAMIL H-BoW had 100\% accuracy and AUC, while mRMR + SVM had 0.81 AUC as reported by Qin et al. \cite{qin041012}. We know CAMIL is not 100\% accurate, so we validated it by averaging the results of 10 independent trials in which we selected 23 patients randomly out of the 367 to serve as the test set, with the other 344 of the training set. Table \ref{tab:test-comp} shows that CAMIL significantly outperformed mRMR + SVM on these trials, with the H-BoW variant of CAMIL slightly outperforming the D-BoW variant.
Unlike the other MIL methods, Qin et al. use reference genomes to inform their classifier, so it makes sense that their method performs better than the MIL methods that only use de novo techniques. However, CAMIL still significantly outperformed the results reported in the MGWAS paper. We believe that the primary reason for this is that Qin et al. relied on alignments of sequences to reference genomes and attempted to select the 50 most significant genes for the phenotype before building the classifier. There are two primary problems with this approach: (i) many microbes found in the gut do not exist in reference databases and would thus be unusable for their classifier, and (ii) by only using 50 genes to inform the classifier, a lot of potentially valuable data is left out. CAMIL avoids these issues by using as much data as can be assembled and not relying on reference databases. The tradeoff is that we don't know exactly what genes are being used by the classifier to form the decision boundary. We also believe that the clustering process of putting similar contigs into groups forms useful features for the classifier.


-----------------------------


Original Table 1:
\begin{table}[h]
\begin{center} 
%\hfill
\caption{Performance with even train/test split.} 
\label{tab:even-comp}
\begin{tabular}{|c|ccc|}\hline
Method & Accuracy & F1-Score & AUC-ROC\\\hline
MISVM & --- & --- & ---\\\hline
sbMIL & --- & --- & ---\\\hline
GICF & 63.04 & 68.33 & 66.19\\\hline %59.24,63.05,66.19
CAMIL D-BoW & 86.34 & 87.18 & 95.93\\\hline
CAMIL H-BoW & \bf{90.71} & \bf{89.70} & \bf{97.63}\\\hline
\end{tabular}
\end{center}
\end{table}

Edited version:
\begin{table}[h]
\begin{center} 
%\hfill
\caption{Performance with even train/test split.} 
\label{tab:even-comp}
\begin{tabular}{|c|ccccc|}\hline
Method & Accuracy & F1-Score & AUC-ROC & CV Acc. & CV F1\\\hline
MISVM & --- & --- & --- & --- & ---\\\hline
sbMIL & --- & --- & --- & --- & ---\\\hline
GICF & 63.04 & 68.33 & 66.19 & --- & ---\\\hline %59.24,63.05,66.19
CAMIL D-BoW & 86.34 & 87.18 & 95.93 & 82.07 & 83.07\\\hline
CAMIL H-BoW & \bf{90.71} & \bf{89.70} & \bf{97.63} & \bf{89.13} & \bf{88.09}\\\hline
\end{tabular}
\end{center}
\end{table}

Added to paragraph explanation:
We also performed Leave One Out Cross Validation for the Accuracy and F1-Score metrics (denoted in the table as CV Acc. and CV F1, respectively). CAMIL's cross validation results were slightly worse than its test set results but still significantly outperformed GICF. Cross validation results for GICF and GICF-cluster were infeasible to calculate due to extremely long computaion time. 


-----------------------------


Not using qsub, execution time:
CAMIL H-BoW: 5 minutes, 25 seconds
CAMIL D-BoW: 5 minutes, 33 seconds
GICF (1 read per cluster): 24 minutes, 51 seconds; Accuracy: 0.793103448276; F1 Score: 0.823529411765; AUC-ROC:  0.82380952381; Memory: 500.07MB


-----------------------------


Also added to explanation of tables 1 and 2:
We attempted to improve on GICF's results by selecting only one read from each cluster for each patient and discarding the other reads.  This significantly reduced the computation time and improved results, showing how important the clustering step is. These results are listed as ``GICF-Cluster" in the tables above. Even with this step, CAMIL was faster and more effective than GICF-Cluster, demonstrating the effectiveness of our feature extraction method.

Edited table 1:
Edited version:
\begin{table}[h]
\begin{center} 
%\hfill
\caption{Performance with even train/test split.} 
\label{tab:even-comp}
\begin{tabular}{|c|ccccc|}\hline
Method & Accuracy & F1-Score & AUC-ROC & CV Acc. & CV F1\\\hline
MISVM & --- & --- & --- & --- & ---\\\hline
sbMIL & --- & --- & --- & --- & ---\\\hline
GICF & 63.04 & 68.33 & 66.19 & --- & ---\\\hline %59.24,63.05,66.19
GICF-Cluster & 79.31 & 82.35 & 82.38 & --- & ---\\\hline
CAMIL D-BoW & 86.34 & 87.18 & 95.93 & 82.07 & 83.07\\\hline
CAMIL H-BoW & \bf{90.71} & \bf{89.70} & \bf{97.63} & \bf{89.13} & \bf{88.09}\\\hline
\end{tabular}
\end{center}
\end{table}

Original table 2: 
\begin{table}[h]
\begin{center}
\caption{Classification time and memory usage with even train/test split.} 
\label{tab:time-comp}
\begin{tabular}{|c|cc|}\hline
Method & Classification Time & Memory Usage\\\hline
MISVM & --- & Memory Error\\\hline
sbMIL & --- & Memory Error\\\hline
GICF & 8 hours, 44 mins, 27 secs & 2.646 GB\\\hline
CAMIL D-BoW & \bf{6 minutes, 58 seconds} & \bf{545.293 MB}\\\hline
CAMIL H-BoW & 7 minutes, 56 seconds & 546.297 MB\\\hline
\end{tabular}
\end{center}
\end{table}

Edited table 2:
\begin{table}[h]
\begin{center}
\caption{Classification time and memory usage with even train/test split.} 
\label{tab:time-comp}
\begin{tabular}{|c|cc|}\hline
Method & Classification Time & Memory Usage\\\hline
MISVM & --- & Memory Error\\\hline
sbMIL & --- & Memory Error\\\hline
GICF & 8 hours, 44 mins, 27 secs & 2.646 GB\\\hline
GICF-Cluster & 24 minutes, 51 secs & \bf{500.07 MB}\\\hline 
CAMIL D-BoW & 5 minutes, 33 seconds & 545.293 MB\\\hline
CAMIL H-BoW & \bf{5 minutes, 25 seconds} & 546.297 MB\\\hline
\end{tabular}
\end{center}
\end{table}


-----------------------------


Removed from methods deiscussion of UCLUST:
Thus, each \emph{centroid} defines the center of a cluster, and the distance T defines the radius of the cluster, such that any point that has a similarity of greater than T to the centroid is within the radius and is thus part of the cluster. UCLUST is a heuristic algorithm that has several optimizations to improve speed, thus condition (i) above is not always guaranteed \cite{Edgar10}. 
