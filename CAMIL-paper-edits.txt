CAMIL Paper Edits Based on Reviews (minor edits not included)


-----------------------------


Original discussion of feature extraction:
We test both the Distance-based and Histogram-based feature extraction methods within the CAMIL pipeline. For each patient, we initialized a vector with length equal to the number of clusters. The value for each scalar in that patient's feature vector would then represent either the read that matched most closely to that cluster centroid (Distance-based), or the number of reads from that cluster that were present (Histogram-based). An example of this is illustrated as a part of Figure \ref{pipeline}. We implemented this feature extraction method in Python. We performed classification with a standard SVM classifier using the generated feature vectors; in this case, we used svm-light \cite{joachims08}. The choice of classifier is not very important for BoW methods \cite{amores13}.


Edited version:
We used a "vocabulary-based" feature extraction method. An example of Vocabulary-based methods are Bag of Words (BoW) methods, which involve the following three-step process: (i) Cluster the instances to create classes of instances; (ii) for each bag, map the clusters of instances in that bag to a feature vector; and (iii) use a standard classifier that uses the feature vectors to predict group labels \cite{amores13}. Step (i) is covered by our assembly and clustering process, while step (iii) is covered by performing classification with a standard SVM classifier based on the extracted feature vectors. In this case, we used svm-light \cite{joachims08}. The choice of classifier is not very important for BoW methods \cite{amores13}. Below, we describe our feature selection methods for step (ii), which we implemented in Python, as well as the rationale for using these methods. 
Amores found the Distance-based Bag of Words (D-BoW) method to be the second most effective of all tested methods, and the most effective one that was also time-efficient (linear, rather than quadratic, in the number of bags and number of instances per bag) \cite{amores13}. H-BoW methods were found by Amores to be somewhat less effective than D-BoW methods on average, but performed the best out of all algorithms on several datasets, indicating that this method performs very well on some real world problems \cite{amores13}. Thus, we tested our pipeline using both of these feature extraction methods. 
Either way, the input is a set of clusters for each patient. The D-BoW method creates a feature vector based on the contig for each cluster that was the closest match to the cluster seed. For instance, say Patient A's reads include the centroid of cluster 1, another contig that has a 45\% match to the centroid of cluster 1, no contigs from cluster 2, and two contigs that match to the centroid of cluster 3, one with a 57\% match and one with an 82\% match. The string match percentage is determined by UCLUST, as described in the previous subsection. Then, D-BoW would extract the feature vector [1, 0, 0.82], indicating the contigs for Patient A that match most closely to the cluster centroid for each cluster. The H-BoW method, instead of using the closest match to each cluster, counts the number of contigs for a patient that belong to each cluster. For the above example, the H-BoW method would extract the feature vector [2, 0, 2], since Patient A has 2 representatives from clusters 1 and 3, but no representatives from cluster 2. This example is illustrated in part of Figure \ref{pipeline}.


Original "Overview" subsection within Methods:
Our proposed pipeline involves a number of steps, which serve a variety of purposes. For each patient file, we assembled the sequence reads, which served the dual purpose of generating larger contigs that contain more functional biological information and reducing the dataset size by discarding reads that could not be assembled. The clustering step assigns the contigs to certain clusters, which represent functionally similar microbes, and thus establish classes of instances that can be used as features for the classifier. We then developed a vocabulary-based feature extraction method.
An example of Vocabulary-based methods are Bag of Words (BoW) methods, which involve the following three-step process: (i) Cluster the instances to create classes of instances; (ii) for each bag, map the clusters of instances in that bag to a feature vector; and (iii) use a standard classifier that uses the feature vectors to predict group labels \cite{amores13}. Amores found the Distance-based Bag of Words (D-BoW) method to be the second most effective of all tested methods, and the most effective one that was also time-efficient (linear, rather than quadratic, in the number of bags and number of instances per bag) \cite{amores13}. The distinguishing feature in D-BoW methods is that the values for the feature vector represent the instance that has the smallest distance to the cluster center. Histogram-based Bag of Words (H-BoW) methods count the number of instances from each cluster there are in each bag, instead of keeping track of the closest-matching instance to that cluster. This intuitively has appeal in the domain of microbiome analysis, as the relative quantities of different species of bacteria is important. H-BoW methods were found by Amores to be somewhat less effective than D-BoW methods on average, but performed the best out of all algorithms on several datasets, indicating that this method performs very well on some real world problems \cite{amores13}.
Our feature extraction method uses either the Histogram-based or Distance-based BoW method. We used an SVM-based classifier to predict patient phenotype, and used several metrics to assess its accuracy. We used the SVM's decision boundary to infer information about which clusters of instances were most or least indicative of the phenotype, discussed further in subsection \ref{deriving-instances}. Aside from the patient labels, this process is entirely de novo, and does not consult any external databases. An illustration of the pipeline is shown in Figure \ref{pipeline} on page \pageref{pipeline}.


Edited version:
Our proposed pipeline involves a number of steps, which serve a variety of purposes. For each patient file, we assembled the sequence reads, which served the dual purpose of generating larger contigs that contain more functional biological information and reducing the dataset size by discarding reads that could not be assembled. The clustering step assigns the contigs to certain clusters, which represent functionally similar microbes, and thus establish classes of instances that can be used as features for the classifier. We then developed a vocabulary-based feature extraction method, discussed further in subsection \ref{feature-extraction}. Using the extracted feature vectors, we trained an SVM-based classifier to predict patient phenotype, and used several metrics to assess its accuracy. We used the SVM's decision boundary to infer information about which clusters of instances were most or least indicative of the phenotype, discussed further in subsection \ref{deriving-instances}. Aside from the patient labels, this process is entirely de novo, and does not consult any external databases. An illustration of the pipeline is shown in Figure \ref{pipeline} on page \pageref{pipeline}.


-----------------------------


Removed from Methods->Assembly section:
SOAPdenovo2 first constructs a type of directed graph called a \emph{de Bruijn} graph that represents the overlaps between different sequences \cite{li10}. Reads are divided into strings of length K called \emph{k-mers}; these k-mers are the nodes of the graph \cite{zerbino08}. The choice of K is up to the user, and is important for having good assembly results. The k-mer nodes in the graph have an edge between them if a read contains those k-mers in order with an overlap of K-1 nucleotides, and the direction of the edge indicates in which order the k-mers appear \cite{zerbino08}. SOAPdenovo2 then cleans this graph by removing nodes/sequences with few or no connections with other sequences, eliminating ``tips" that represent likely sequencing machine errors, and removing redundant edges \cite{li10}. This step helps to reduce the overall size of the data by eliminating some reads that would not have been useful anyway. The contigs are then formed by combining reads according to the de Bruijn graph: each contig represents a directed path in the graph \cite{zerbino08}. 


-----------------------------



