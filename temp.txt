Original 60: SRR413*, where * is 556-557, 646-664, 666-682, 684-702, 757-759

--------------------------------

python pipelineT2D.py -v --dir /home/nathan/school/research/multiInstanceLearning/cirrhosisTests --input cirrhosis.fasta --map cirrhosis.map --positive Encephalopathy --negative 'No Encephalopathy' --cluster uclust --svm misvm --result_dir results/option4-misvm-encVSno/ --output resultFile --split -1 --kmer 4 --write small_vectors

python gicf.py -v -i /home/nathan/school/research/multiInstanceLearning/cirrhosisTests/results/option4-misvm-encVSno/vectors

--------------------------------

K = 41 or K = 51 with read size limited to 100 seemed to work best. I chose K = 51.

--------------------------------

GeneMark: done, not sure how to use yet

Explicit association of clusters to labels with SVM-light
- take the prediction file and augment with explicit association with cluster labels and an example
- maybe augment with genemark results to predict genes that are most indicative of having or not having disease

Start writing my paper
- clarify direction of paper
- start with parts of introduction and related work sections from class paper, maybe some of methods (UCLUST, SVM-light, etc)
- remember matplotlib for charts: http://matplotlib.org/

Same data as MGWAS paper?
- can start downloading other patients with bulk downloader, but still not all data is available
- may have to ask paper authors where to find all data

--------------------------------

EXPERIMENTS for GICF (accuracy and F1 [maybe also runtime])

Number of iterations: 1, 3, 5, 10, 20
Mini-batch size: 0.01, 0.05, 0.1, 0.25, 1
Selected from: Top 2%, Top 10%, Top 20%, Top 50%, All
Learning Rate: 0.00001, 0.00005, 0.0001, 0.0005, 0.001
Top-K: Yes or No
Kernel (Similarity): Yes or No
Part of Dataset: 1% or All

Since mini batches are randomized (as are weights), run each experiment 3-5 times to get range of results
Do training set as test set to see accuracy of that
Do both Cirrhosis and T2D datasets (must first assemble Cirrhosis data)

--------------------------------

  /* added code */
  int iter;
  double *wtemp = model->lin_weights;
  fprintf(predfl, "[");
  for(iter=0; iter < len; iter++){
    fprintf(predfl,"%lf", *wtemp);
    wtemp++;
    if(iter+1 < len)  fprintf(predfl,", ");
  }
  fprintf(predfl, "]\n");
  /* end added code */

1317: number of clusters from assembled fasta file

--------------------------------

k=4, out1
Best parameter settings [learn rate, lambda, mini batch size, descent iterations]: [0.0001, 60.0, 0.9, 10]
Average prediction: 0.762984045957
Best results: 
Accuracy: 0.633333333333
Precision and recall: 0.590909090909 and 0.866666666667
F1 Score: 0.702702702703
True Positive Rate and False Positive Rate: 0.866666666667 and 0.6


k=3, out1
Best parameter settings [rate, lambda, top_k, mini batch size, iterations]:
[0.0001, 60.0, 0.9, 0.01, 30]
Best results: 
Accuracy: 0.6
Precision and recall: 0.555555555556 and 1.0
F1 Score: 0.714285714286
True Positive Rate and False Positive Rate: 1.0 and 0.8


k=3, out1
Best parameter settings [rate, lambda, top_k, mini batch size, iterations]:
[5e-05, 60.0, 0.9, 0.01, 30]
Best results: 
Accuracy: 0.633333333333
Precision and recall: 0.583333333333 and 0.933333333333
F1 Score: 0.717948717949
True Positive Rate and False Positive Rate: 0.933333333333 and 0.666666666667


k=3, out1
Best parameter settings [rate, lambda, top_k, mini batch size, iterations]:
[0.0001, 60.0, 0.9, 0.01, 3]
Best results: 
Accuracy: 0.7
Precision and recall: 0.6875 and 0.733333333333
F1 Score: 0.709677419355
True Positive Rate and False Positive Rate: 0.733333333333 and 0.333333333333

k=3, out1
Best parameter settings [rate, lambda, top_k, mini batch size, iterations]:
[0.0001, 60.0, 0.9, 0.01, 30]
Best results: 
Accuracy: 0.566666666667
Precision and recall: 0.535714285714 and 1.0
F1 Score: 0.697674418605
True Positive Rate and False Positive Rate: 1.0 and 0.866666666667




--------------------------------

558-567
568-577
578-587
588-597
598-607
608-617
618-627
628-637
638-645
665
683
703-712
713-722
723-732
733-742
743-756
760-773


572-577
624-627
738-742
755-756
763-767
768-773


--768

